{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch CNN (separate)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pigsoft/Test/blob/master/PyTorch_CNN_(separate).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1oZ7s28jTUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "torch.manual_seed(0)\n",
        "dtype = torch.double\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNdyq0ep2ex5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class all_activation_function:\n",
        "#     def __init__(self):\n",
        "#         pass\n",
        "        \n",
        "#     def Relu(self, x) :\n",
        "#         return x*(x>0)\n",
        "    \n",
        "#     def d_Relu(self, x) :\n",
        "#         return 1.0*(x>0)\n",
        "    \n",
        "#     def PRelu(self, x, alpha) :\n",
        "#         return x*(x>=0) + alpha*x*(x<0)\n",
        "    \n",
        "#     def d_PRelu(self, x, alpha) :\n",
        "#         return 1.0*(x>=0) + alpha*(x<0)\n",
        "    \n",
        "#     def Tanh(self, x) :\n",
        "#         return torch.tanh(x)\n",
        "    \n",
        "#     def d_Tanh(self, x) :\n",
        "#         return 1 - torch.tanh(x)**2\n",
        "    \n",
        "#     def sigmoid(self, x) :\n",
        "#         return 1.0/(1 + torch.exp(-x))\n",
        "    \n",
        "#     def d_sigmoid(self, x) :\n",
        "#         tmp = torch.exp(-x)\n",
        "#         return tmp/((1-tmp)**2)\n",
        "    \n",
        "#     def Elu(self, x, alpha) :\n",
        "#         return alpha*(torch.exp(x)-1)*(x<=0) + x*(x>0)\n",
        "    \n",
        "#     def d_Elu(self, x, alpha) :\n",
        "#         return alpha * torch.exp(x) * (x<=0) + 1.0 * (x>0)\n",
        "    \n",
        "#     def softmax(self, x):\n",
        "#         return torch.exp(x) / torch.reshape(torch.sum(torch.exp(x), axis=1), (-1, 1))\n",
        "    \n",
        "#     def log_softmax(self, x):\n",
        "#         return torch.log(self.softmax(x))\n",
        "    \n",
        "#     def CrossEntropy(self, x, y):\n",
        "#         log_softmax = -self.log_softmax(x)\n",
        "        \n",
        "#         expected_loss = torch.zeros(1, dtype=dtype)\n",
        "#         for row, col in enumerate(y):\n",
        "#             expected_loss += log_softmax[row, col]\n",
        "#         expected_loss /= log_softmax.shape[0]\n",
        "\n",
        "#         return expected_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h60wYsN4uJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.W = torch.randn(in_features, out_features, device=device, dtype=dtype, requires_grad=True)\n",
        "        self.b = torch.randn(out_features, device=device, dtype=dtype, requires_grad=True)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return X @ W + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G55p5kIOlePv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv2d:\n",
        "    # input – input tensor of shape (minibatch,in_channels,iH,iW)\n",
        "    # weight – filters of shape (out_channels, in_channels, kH, kW)\n",
        "    # bias – optional bias tensor of shape (out_channels). Default: None\n",
        "    # stride – the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1\n",
        "    # padding – implicit paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0\n",
        "\n",
        "    # The parameters kernel_size, stride, padding, dilation can either be:\n",
        "    #     a single int – in which case the same value is used for the height and width dimension\n",
        "    #     a tuple of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension\n",
        "\n",
        "    # In the simplest case, the output value of the layer with input size (N, C_in, H, W) and output (N, C_out, H_out, W_out)\n",
        "    # weight – filters of shape (C_out, C_in, kH, kW) \n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        self.in_channels  = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size  = kernel_size\n",
        "        self.stride       = stride\n",
        "        self.padding      = padding\n",
        "\n",
        "        if isinstance(kernel_size, int):\n",
        "            self.kernel_size = (kernel_size, kernel_size)\n",
        "        if isinstance(stride, int):\n",
        "            self.stride = (stride, stride)\n",
        "        \n",
        "        self.W = torch.randn(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1], device=device, dtype=dtype, requires_grad=True)\n",
        "        self.b = torch.randn(self.out_channels, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.padding > 0:\n",
        "            # Add padding\n",
        "            left_right = torch.zeros((X.shape[0], self.padding))\n",
        "            X = torch.cat((left_right, X, left_right), 1)\n",
        "            top_down   = torch.zeros((self.padding, X.shape[1]))\n",
        "            X = torch.cat((top_down, X, top_down), 0)\n",
        "\n",
        "\n",
        "        print('conv2d PyTorch')\n",
        "        print(F.conv2d(X.clone(), self.W.clone(), bias=self.b.clone()))\n",
        "        print()\n",
        "\n",
        "        X_minibatch = X.shape[0]\n",
        "        X_dim = X.shape[1]\n",
        "        X_row = X.shape[2]\n",
        "        X_col = X.shape[3]\n",
        "\n",
        "        X_out_minibatch = X.shape[0]\n",
        "        X_out_dim = self.out_channels\n",
        "        X_out_row = int((X_row - self.kernel_size[0] + 2 * self.padding) / self.stride[0] + 1)\n",
        "        X_out_col = int((X_col - self.kernel_size[1] + 2 * self.padding) / self.stride[1] + 1)\n",
        "\n",
        "        X_out = torch.zeros((X_out_minibatch, X_out_dim, X_out_row, X_out_col))\n",
        "\n",
        "        for n in range(X_out_minibatch):\n",
        "            for f in range(X_out_dim):\n",
        "                for i in range(self.kernel_size[0], X_row + 1, self.stride[0]):\n",
        "                    for j in range(self.kernel_size[1], X_col + 1, self.stride[1]):\n",
        "                        # print('({}, {})'.format(i, j))\n",
        "                        # print('X')\n",
        "                        # print(X[n, :, i - self.kernel_size[0], j - self.kernel_size[1]])\n",
        "                        # print('W')\n",
        "                        # print(self.W[f, ...])\n",
        "                        X_out_i = torch.sum(X[n, :, i - self.kernel_size[0]: i, j - self.kernel_size[1]: j] * self.W[f, ...]) + self.b[f]\n",
        "                        X_out[n][f][i - self.kernel_size[0]][j - self.kernel_size[1]] = X_out_i\n",
        "\n",
        "        # # i-th mini-batch\n",
        "        # for id_X, X_i in enumerate(X):\n",
        "        #     # i-th filter\n",
        "        #     for id_W, W_i in enumerate(self.W):\n",
        "        #         # Convolute X_i and i-th filter\n",
        "        #         print(\"X_i\")\n",
        "        #         print(X_i.shape)\n",
        "        #         print(X_i)\n",
        "        #         print(\"W_i\")\n",
        "        #         print(W_i.shape)\n",
        "        #         print(W_i)\n",
        "        #         for i in range(self.kernel_size[0], X_row + 1, self.stride[0]):\n",
        "        #             for j in range(self.kernel_size[1], X_col + 1, self.stride[1]):\n",
        "        #                 print('i =', i - self.kernel_size[0], i)\n",
        "        #                 print('j =', j - self.kernel_size[1], j)\n",
        "        #                 print(X_i[i - self.kernel_size[0]: i, j - self.kernel_size[1]: j] * W_i)\n",
        "        #                 X_out_i = torch.sum(X_i[i - self.kernel_size[0]: i, j - self.kernel_size[1]: j] * W_i) + b[id_W]\n",
        "        #                 X_out[id_X][id_W][i - self.kernel_size[0]][j - self.kernel_size[1]] = X_out_i     \n",
        "        return X_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaBjgWEu4h1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class MaxPool2d:\n",
        "#     def __init__(self, X, kernel_size, stride=0, padding=0):\n",
        "#         self.kernel_size  = kernel_size\n",
        "#         self.stride       = stride\n",
        "#         self.padding      = padding\n",
        "\n",
        "#         if isinstance(kernel_size, int):\n",
        "#             kernel_size = (kernel_size, kernel_size)\n",
        "#         if isinstance(stride, int):\n",
        "#             stride = (stride, stride)\n",
        "\n",
        "#     def forward(self, X):\n",
        "#         if slef.padding > 0:\n",
        "#             # Add padding\n",
        "#             left_right = torch.zeros((X.shape[0], padding))\n",
        "#             X = torch.cat((left_right, X, left_right), 1)\n",
        "#             top_down   = torch.zeros((padding, X.shape[1]))\n",
        "#             a = torch.cat((top_down, X, top_down), 0)\n",
        "        \n",
        "#         X_minibatch = X.shape[0]\n",
        "#         X_dim = X.shape[1]\n",
        "#         X_row = X.shape[2]\n",
        "#         X_col = X.shape[3]\n",
        "\n",
        "#         X_out_minibatch = X.shape[0]\n",
        "#         X_out_dim = X.shape[1]\n",
        "#         X_out_row = int((X_row - kernel_size[0] + 2 * padding) / stride[0] + 1)\n",
        "#         X_out_col = int((X_col - kernel_size[1] + 2 * padding) / stride[1] + 1)\n",
        "\n",
        "#         X_out = torch.zero((X_out_minibatch, X_out_dim, X_out_row, X_out_col))\n",
        "        \n",
        "#         # i-th mini-batch\n",
        "#         for id_X, X_i in enumerate(X):\n",
        "#             # i-th dimension\n",
        "#             for id_X_dim in range(X_dim):\n",
        "#                 # max_pool2d\n",
        "#                 for i in range(kernel_size[0], X_row + 1, self.stride[0]):\n",
        "#                     for j in range(kernel_size[1], X_col + 1, self.stride[1]):\n",
        "#                         X_out_i = torch.sum(X_i[i - kernel_size[0]: i, j - kernel_size[1]: j] * W_i) + b[id_W]\n",
        "#                         X_out[id_X][id_X_dim][i - kernel_size[0]][j - kernel_size[1]] = X_out_i     \n",
        "#         return X_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTJ0sIWEbg2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class nn:\n",
        "    def __init__(self):\n",
        "        self.weight = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSSby3K2ePCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "954b27a9-eb86-41bd-d21a-b6955af8859d"
      },
      "source": [
        "# Test Conv2d\n",
        "A = torch.arange(2.0 * 1.0 * 3.0 * 3.0, dtype=dtype).reshape(2, 1, 3, 3)\n",
        "print(A)\n",
        "print()\n",
        "conv2d = Conv2d(1, 2, 2)\n",
        "\n",
        "print(\"Convolute\")\n",
        "conv1 = conv2d.forward(A)\n",
        "print(conv1.shape)\n",
        "print(conv1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.,  1.,  2.],\n",
            "          [ 3.,  4.,  5.],\n",
            "          [ 6.,  7.,  8.]]],\n",
            "\n",
            "\n",
            "        [[[ 9., 10., 11.],\n",
            "          [12., 13., 14.],\n",
            "          [15., 16., 17.]]]], dtype=torch.float64)\n",
            "\n",
            "Convolute\n",
            "conv2d PyTorch\n",
            "tensor([[[[ -5.2753,  -5.6381],\n",
            "          [ -6.3637,  -6.7265]],\n",
            "\n",
            "         [[  2.7602,   1.5185],\n",
            "          [ -0.9650,  -2.2068]]],\n",
            "\n",
            "\n",
            "        [[[ -8.5404,  -8.9032],\n",
            "          [ -9.6288,  -9.9916]],\n",
            "\n",
            "         [[ -8.4155,  -9.6572],\n",
            "          [-12.1407, -13.3825]]]], dtype=torch.float64,\n",
            "       grad_fn=<ThnnConv2DBackward>)\n",
            "\n",
            "torch.Size([2, 2, 2, 2])\n",
            "tensor([[[[ -5.2753,  -5.6381],\n",
            "          [ -6.3637,  -6.7265]],\n",
            "\n",
            "         [[  2.7602,   1.5185],\n",
            "          [ -0.9650,  -2.2068]]],\n",
            "\n",
            "\n",
            "        [[[ -8.5404,  -8.9032],\n",
            "          [ -9.6288,  -9.9916]],\n",
            "\n",
            "         [[ -8.4155,  -9.6572],\n",
            "          [-12.1407, -13.3825]]]], grad_fn=<CopySlices>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHaIcH4_TVo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class neural_network(all_activation_function):\n",
        "\n",
        "#     def __init__(self, X, y):\n",
        "#         self.X_train = torch.from_numpy(X).to(device=device)\n",
        "#         self.X = torch.from_numpy(X).to(device=device)\n",
        "#         self.y = torch.from_numpy(y)\n",
        "#         self.W = []\n",
        "#         self.b = []\n",
        "#         self.n_node = []\n",
        "#         self.activation_func = []\n",
        "#         self.loss_func = None\n",
        "#         self.list_loss = []\n",
        "#         self.loss_ = None\n",
        "\n",
        "#         self.para_conv = []\n",
        "#         self.para_pool = []\n",
        "#         self.para_flatten = []\n",
        "\n",
        "#         self.order_node = []\n",
        "\n",
        "#         self.dict_activation_func = {\"sigmoid\": self.sigmoid,\n",
        "#                                      \"softmax\": self.softmax,\n",
        "#                                      \"softmax_cross_entropy\": self.CrossEntropy\n",
        "#                                      }\n",
        "\n",
        "#     def linear(self, X, W, b):\n",
        "#         Z = X @ W + b\n",
        "#         return Z\n",
        "    \n",
        "#     def conv2d(self, X, W, b, filters, kernel_size, strides, padding):\n",
        "#         if padding > 0:\n",
        "#             # Add padding\n",
        "#             left_right = torch.zeros((X.shape[0], padding))\n",
        "#             X = torch.cat((left_right, X, left_right), 1)\n",
        "#             top_down   = torch.zeros((padding, X.shape[1]))\n",
        "#             a = torch.cat((top_down, X, top_down), 0)\n",
        "\n",
        "#         X_dims = X.shape[0]\n",
        "#         X_rows = X.shape[1]\n",
        "#         X_cols = X.shape[2]\n",
        "#         X_out = torch.tensor([])\n",
        "\n",
        "#         X_out_dims = kernel_size\n",
        "#         X_out_rows = int((X_rows - filters + 2 * padding) / strides + 1)\n",
        "#         X_out_cols = int((X_cols - filters + 2 * padding) / strides + 1)\n",
        "\n",
        "#         # i-th filter\n",
        "#         for W_i in W:\n",
        "#             # Convolute X and i-th filter\n",
        "#             for i in range(kernel_size, X_rows + 1, strides):\n",
        "#                 for j in range(kernel_size, X_cols + 1, strides):\n",
        "#                     X_out_i = torch.sum(X[i - kernel_size: i, j - kernel_size: j] * W_i) + b\n",
        "#                     X_out = torch.cat((X_out, X_out_i))\n",
        "\n",
        "#         X_out = X_out.reshape(X_out_dims, X_out_rows, X_out_cols)\n",
        "    \n",
        "#     def maxpool2d(self, X, kernel_size, strides, padding=0):\n",
        "#         if padding > 0:\n",
        "#             # Add padding\n",
        "#             left_right = torch.zeros((X.shape[0], padding))\n",
        "#             X = torch.cat((left_right, X, left_right), 1)\n",
        "#             top_down   = torch.zeros((padding, X.shape[1]))\n",
        "#             a = torch.cat((top_down, X, top_down), 0)\n",
        "        \n",
        "#         X_dims = X.shape[0]\n",
        "#         X_rows = X.shape[1]\n",
        "#         X_cols = X.shape[2]\n",
        "#         X_out = torch.tensor([])\n",
        "\n",
        "#         X_out_dims = X_dims\n",
        "#         X_out_rows = int((X_rows - filters + 2 * padding) / strides + 1)\n",
        "#         X_out_cols = int((X_cols - filters + 2 * padding) / strides + 1)\n",
        "\n",
        "#         # i-th filter\n",
        "#         for W_i in W:\n",
        "#             # Convolute X and i-th filter\n",
        "#             for i in range(kernel_size, X_rows + 1, strides):\n",
        "#                 for j in range(kernel_size, X_cols + 1, strides):\n",
        "#                     X_out_i = torch.max(X[i - kernel_size: i, j - kernel_size: j])\n",
        "\n",
        "#     def fc(self, n_node, activation_func):\n",
        "#         self.n_node.append(n_node)\n",
        "#         self.activation_func.append(activation_func)\n",
        "    \n",
        "#     def conv(self, filters, kernel_size, strides=1, padding=0, activation='Relu'):\n",
        "        \n",
        "\n",
        "#     def loss(self, n_node, loss_func):\n",
        "#         self.n_node.append(n_node)\n",
        "#         self.loss_func = loss_func\n",
        "    \n",
        "#     def forward_train(self, epoch):\n",
        "#         # Set X_train to original value for forward \n",
        "#         self.X = self.X_train\n",
        "\n",
        "#         # i = 0 (first layer) to len(self.n_node) - 1 \n",
        "#         for i in range(len(self.n_node) - 1):\n",
        "#             n_prev_node = self.X.shape[1]\n",
        "#             n_curr_node = self.n_node[i]\n",
        "\n",
        "#             Z = self.linear(self.X, self.W[i], self.b[i])\n",
        "#             self.X = self.dict_activation_func[self.activation_func[i]](Z)\n",
        "        \n",
        "#         # Last layer (loss function)        \n",
        "#         Z = self.linear(self.X, self.W[-1], self.b[-1])\n",
        "#         self.loss = self.dict_activation_func[self.loss_func](Z, self.y)\n",
        "#         self.list_loss.append(self.loss.data)\n",
        "        \n",
        "#         if epoch % 100 == 0:\n",
        "#             print('epoch: {}, \\tloss: {}'.format(epoch, self.loss.data[0]))\n",
        "    \n",
        "#     def forward_test(self):\n",
        "#         # Set X_train to original value for forward \n",
        "#         self.X = self.X_test\n",
        "\n",
        "#         # i = 0 (first layer) to len(self.n_node) - 1 \n",
        "#         for i in range(len(self.n_node) - 1):\n",
        "#             n_prev_node = self.X.shape[1]\n",
        "#             n_curr_node = self.n_node[i]\n",
        "            \n",
        "#             Z = self.linear(self.X, self.W[i], self.b[i])\n",
        "#             self.X = self.dict_activation_func[self.activation_func[i]](Z)\n",
        "        \n",
        "#         # Last layer (loss function)        \n",
        "#         Z = self.linear(self.X, self.W[-1], self.b[-1])\n",
        "#         predicted = torch.argmax(Z, axis=1)\n",
        "#         return predicted\n",
        "\n",
        "#     def backward(self, lr):\n",
        "#         # Backpropagation\n",
        "#         self.loss.backward()\n",
        "#         for i in range(len(self.W)):\n",
        "#             with torch.no_grad():\n",
        "#                 self.W[i] -= self.W[i].grad * lr\n",
        "#                 self.b[i] -= self.b[i].grad * lr\n",
        "\n",
        "#                 self.W[i].grad.zero_()\n",
        "#                 self.b[i].grad.zero_()\n",
        "\n",
        "#     def fit(self, n_epoch=2000, lr=1e-6):\n",
        "#         # Create weights and biases\n",
        "#         n_prev_node = self.X_train.shape[1]\n",
        "#         for i in range(len(self.n_node)):\n",
        "#             n_curr_node = self.n_node[i]\n",
        "            \n",
        "#             torch.manual_seed(0)\n",
        "#             W = torch.randn(n_prev_node, n_curr_node, device=device, dtype=dtype, requires_grad=True)\n",
        "#             b = torch.randn(n_curr_node, device=device, dtype=dtype, requires_grad=True)\n",
        "#             self.W.append(W)\n",
        "#             self.b.append(b)\n",
        "\n",
        "#             n_prev_node = self.n_node[i]\n",
        "        \n",
        "#         for epoch in range(n_epoch):\n",
        "#             self.forward_train(epoch)\n",
        "#             self.backward(lr)\n",
        "\n",
        "#     def predict(self, X_test):\n",
        "#         self.X_test = torch.from_numpy(X_test)\n",
        "#         predicted = self.forward_test()\n",
        "#         return predicted\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aecfau48j45z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.datasets import load_digits\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# digits = load_digits()\n",
        "# X = np.array(digits.data)\n",
        "# y = np.array(digits.target)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)\n",
        "# print(y_train.shape)\n",
        "# print(y_test.shape)\n",
        "# print(type(X_train[0][0]))\n",
        "# print(type(y_train[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYaWhHkmF_Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn = neural_network(X_train, y_train)\n",
        "# nn.fc(10, 'sigmoid')\n",
        "# nn.fc(10, 'sigmoid')\n",
        "# nn.fc(10, 'sigmoid')\n",
        "# nn.loss(10, 'softmax_cross_entropy')\n",
        "# nn.fit(n_epoch=1001, lr=1.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4i0dAzNBxXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# y_predicted = nn.predict(X_test)\n",
        "# acc = accuracy_score(y_test, y_predicted)\n",
        "# print('Accuracy:', acc * 100.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubjia2mOFGGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "# torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LrkLFP7TliG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = torch.tensor([[1., 2.], [3., 4.], [5., 6.]])\n",
        "# # print(a)\n",
        "# l = torch.zeros((a.shape[0], 1))\n",
        "# print(l)\n",
        "# a = torch.cat((l, a, l), 1)\n",
        "# print(a)\n",
        "\n",
        "# r = torch.zeros((1, a.shape[1]))\n",
        "# a = torch.cat((r, a, r), 0)\n",
        "# print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NOiaCt-f5RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = []\n",
        "# b = [1, 2, 3]\n",
        "# print(not a)\n",
        "# print(not b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3oHlKayiRfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = torch.tensor([[[1], [2]], [[3], [4]], [[5], [6]]])\n",
        "# print(a.shape)\n",
        "# print()\n",
        "# print(a[0])\n",
        "# print(a[1])\n",
        "# print(a[2])\n",
        "# a.reshape(-1)\n",
        "# print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfBWleTFj89v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in a:\n",
        "#     print(i)\n",
        "#     print()\n",
        "#     print(i.shape)\n",
        "#     print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCpRepholkpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_out = torch.tensor([])\n",
        "# X_out = torch.cat((X_out, torch.tensor([1.])))\n",
        "# print(X_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLz0e-AE97Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch\n",
        "\n",
        "# a = torch.tensor([[1, 2], [3, 4]])\n",
        "# print(torch.max(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOPKfOPeITxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import math\n",
        "# class ts:\n",
        "#     def __init__(self):\n",
        "#         self.data = 1.0\n",
        "#     def __log__(self):\n",
        "#         print(self.x)\n",
        "# t = ts()\n",
        "# print(math.log(t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1UWXfk7LWac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.tensor([1, 2, 3, -1, -2])\n",
        "# print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAT7917AbBm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from scipy import signal\n",
        "\n",
        "# A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "# F = np.array([[1, 2], [3, 4]])\n",
        "# print(signal.convolve2d(A, F))\n",
        "# print(signal.correlate2d(A, F))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLbDTYju1aNg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e5b60260-d11d-4730-cf66-7f328f719e19"
      },
      "source": [
        "a = torch.arange(2.0 * 3.0 * 4.0).reshape(2, 3, 4)\n",
        "print(a[1, ...].clone())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.],\n",
            "        [20., 21., 22., 23.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBkVZiiG8VDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}